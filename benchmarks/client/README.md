## Test client locally

Starting vllm server:


```shell
export API_KEY=${API_KEY}
python3 -m vllm.entrypoints.openai.api_server --host 0.0.0.0 \
--port "8000" \
--model /root/models/deepseek-llm-7b-chat \
--trust-remote-code \
--max-model-len "4096" \
--api-key ${API_KEY} \
--enable-chunked-prefill
```

Using a sample workload (generated by [the workload generator](../generator/README.md)) in a client. Turn on `--streaming` to collect fine grained metrics such as `TTFT` and  `TPOT`:

```shell
export API_KEY=${API_KEY}
python3 client.py \
--workload-path "../generator/output/constant.jsonl" \
--endpoint "http://localhost:8000" \
--model /root/models/deepseek-llm-7b-chat \
--api-key ${API_KEY} \
--streaming \
--output-file-path output.jsonl 
```
The output will be stored as a `.jsonl` file in `output.jsonl`

Run analysis on metrics collected. For streaming client, we can specify a goodput target (e2e/tpot/ttft) like the following: 

```shell
python analyze.py --trace output.jsonl --output output --goodput-target tpot:0.5
```
